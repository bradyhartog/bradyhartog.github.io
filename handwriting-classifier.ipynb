{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handwriting-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnezLWMfELVw"
      },
      "source": [
        "# **Handwriting Classifier**\n",
        "**From “Neural Networks and Deep Learning,” by Michael A. Nielsen**\n",
        "\n",
        "<br>\n",
        "A simple module to implement the stochastic gradient descent learning algorithm\n",
        "for a feedforward neural network.\n",
        "\n",
        "We evaluate the implementation as a classifier for handwritten digits (0–9).\n",
        "Training and testing data is from the MNIST data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV4OyLBUHAQf"
      },
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1-elLOlHP5A"
      },
      "source": [
        "'''\n",
        "A simple feedforward neural network which uses the stochastic gradient descent\n",
        "learning algorithm. Gradients are computed using backpropagation.\n",
        "'''\n",
        "class Network(object):\n",
        "  '''\n",
        "  Initialize the Network using random weights and biases. (There exist better\n",
        "  ways of initializing weights and biases.) Assume the first layer of neurons is\n",
        "  an input layer and do not set biases for this layer.\n",
        "  '''\n",
        "  def __init__(self, sizes):\n",
        "    self.numLayers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [\n",
        "      np.random.randn(y, 1) for y in sizes[1:]\n",
        "    ]\n",
        "    self.weights = [\n",
        "      np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])\n",
        "    ]\n",
        "\n",
        "  '''\n",
        "  Compute the output of the network for the input vector a.\n",
        "  '''\n",
        "  def feedforward(self, a):\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "\n",
        "    return a\n",
        "\n",
        "  '''\n",
        "  Train the neural network using mini-batch stochastic gradient descent.\n",
        "  '''\n",
        "  def SGD(self, trainingData, epochs, miniBatchSize, eta, testData=None):\n",
        "    if testData:\n",
        "      nTest = len(testData)\n",
        "\n",
        "    n = len(trainingData)\n",
        "\n",
        "    for j in range(epochs):\n",
        "      random.shuffle(trainingData)\n",
        "\n",
        "      miniBatches = [\n",
        "        trainingData[k:(k + miniBatchSize)] for k in range(0, n, miniBatchSize)\n",
        "      ]\n",
        "\n",
        "      for miniBatch in miniBatches:\n",
        "        self.updateMiniBatch(miniBatch, eta)\n",
        "\n",
        "      if testData:\n",
        "        print(\"Epoch {0} : {1} / {2}\".format(j, self.evaluate(testData), nTest))\n",
        "      else:\n",
        "        print(\"Epoch {0} complete.\".format(j))\n",
        "\n",
        "  '''\n",
        "  Update the network's weights and biases by applying gradient descent using\n",
        "  backpropagation to a single mini batch.\n",
        "  '''\n",
        "  def updateMiniBatch(self, miniBatch, eta):\n",
        "    Vb = [np.zeros(b.shape) for b in self.biases]  # \"del-b\"\n",
        "    Vw = [np.zeros(w.shape) for w in self.weights] # \"del-w\"\n",
        "\n",
        "    for x, y in miniBatch:\n",
        "      dVb, dVw = self.backprop(x, y) # \"delta-del-b,\" \"delta-del-w\"\n",
        "\n",
        "      Vb = [nb+dnb for nb, dnb in zip(Vb, dVb)]\n",
        "      Vw = [nw+dnw for nw, dnw in zip(Vw, dVw)]\n",
        "\n",
        "    self.weights = [\n",
        "      w - nw*(eta/len(miniBatch)) for w, nw in zip(self.weights, Vw)\n",
        "    ]\n",
        "    self.biases = [\n",
        "      b - nb*(eta/len(miniBatch)) for b, nb in zip(self.biases, Vb)\n",
        "    ]\n",
        "\n",
        "  '''\n",
        "  Compute a tuple (Vb, Vw) representing the gradient of the cost function.\n",
        "\n",
        "  Vb and Vw (\"del-b and del-w\") are layer-by-layer lists of numpy arrays,\n",
        "  similar to self.biases and self.weights.\n",
        "  '''\n",
        "  def backprop(self, x, y):\n",
        "    Vb = [np.zeros(b.shape) for b in self.biases]\n",
        "    Vw = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    '''\n",
        "    Feedforward\n",
        "    '''\n",
        "\n",
        "    activation = x\n",
        "    activations = [x] # A list to store all activations, layer by layer.\n",
        "    zs = []           # A list to store all z vectors, layer by layer.\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      z = np.dot(w, activation) + b\n",
        "      zs.append(z)\n",
        "      activation = sigmoid(z)\n",
        "      activations.append(activation)\n",
        "\n",
        "    '''\n",
        "    Backward Pass\n",
        "    '''\n",
        "\n",
        "    delta = self.costDerivative(activations[-1], y) * sigmoidPrime(zs[-1])\n",
        "\n",
        "    Vb[-1] = delta\n",
        "    Vw[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    # l = 1 ==> last layer, l = 2 ==> second-last layer, and so on.\n",
        "    for l in range(2, self.numLayers):\n",
        "      z = zs[-l]\n",
        "      sp = sigmoidPrime(z)\n",
        "\n",
        "      delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "\n",
        "      Vb[-l] = delta\n",
        "      Vw[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    \n",
        "    return (Vb, Vw)\n",
        "\n",
        "  '''\n",
        "  Compute the number of test inputs for which the neural network outputs the\n",
        "  correct result.\n",
        "\n",
        "  Assume the output is the index of whichever neuron in the final layer has\n",
        "  the highest activation.\n",
        "  '''\n",
        "  def evaluate(self, testData):\n",
        "    testResults = [(np.argmax(self.feedforward(x)), y) for (x, y) in testData]\n",
        "\n",
        "    return sum(int(x == y) for (x, y) in testResults)\n",
        "\n",
        "  '''\n",
        "  Compute the vector of partial derivatives (of the cost function with respect\n",
        "  to activation a) for the output activations.\n",
        "  '''\n",
        "  def costDerivative(self, outputActivations, y):\n",
        "    return (outputActivations - y)\n",
        "\n",
        "'''\n",
        "GENERAL HELPER FUNCTIONS\n",
        "'''\n",
        "\n",
        "'''\n",
        "Compute the sigmoid function of the real number z.\n",
        "'''\n",
        "def sigmoid(z):\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "'''\n",
        "Compute the derivative of the sigmoid function of the real number z.\n",
        "'''\n",
        "def sigmoidPrime(z):\n",
        "  return sigmoid(z) * (1 - sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnZjtQO4NRRh"
      },
      "source": [
        "### **Training and Evaluation**\n",
        "\n",
        "Train and evaluate the neural network as a handwriting classifier using images from the MNIST data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mnFp4LjTuOt"
      },
      "source": [
        "import mnist_loader\n",
        "trainingData, validationData, testData = mnist_loader.load_data_wrapper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seA7zN2XTwdF",
        "outputId": "8240632e-f1bd-44e2-c03f-fe2786051742"
      },
      "source": [
        "net = Network([784, 30, 10])\n",
        "\n",
        "net.SGD(list(trainingData), 30, 10, 3.0, testData=list(testData))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : 9157 / 10000\n",
            "Epoch 1 : 9284 / 10000\n",
            "Epoch 2 : 9289 / 10000\n",
            "Epoch 3 : 9353 / 10000\n",
            "Epoch 4 : 9321 / 10000\n",
            "Epoch 5 : 9418 / 10000\n",
            "Epoch 6 : 9452 / 10000\n",
            "Epoch 7 : 9469 / 10000\n",
            "Epoch 8 : 9467 / 10000\n",
            "Epoch 9 : 9468 / 10000\n",
            "Epoch 10 : 9508 / 10000\n",
            "Epoch 11 : 9501 / 10000\n",
            "Epoch 12 : 9487 / 10000\n",
            "Epoch 13 : 9513 / 10000\n",
            "Epoch 14 : 9508 / 10000\n",
            "Epoch 15 : 9537 / 10000\n",
            "Epoch 16 : 9541 / 10000\n",
            "Epoch 17 : 9526 / 10000\n",
            "Epoch 18 : 9536 / 10000\n",
            "Epoch 19 : 9541 / 10000\n",
            "Epoch 20 : 9522 / 10000\n",
            "Epoch 21 : 9514 / 10000\n",
            "Epoch 22 : 9543 / 10000\n",
            "Epoch 23 : 9539 / 10000\n",
            "Epoch 24 : 9546 / 10000\n",
            "Epoch 25 : 9538 / 10000\n",
            "Epoch 26 : 9544 / 10000\n",
            "Epoch 27 : 9533 / 10000\n",
            "Epoch 28 : 9548 / 10000\n",
            "Epoch 29 : 9533 / 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5qjkmWNVk3P"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}